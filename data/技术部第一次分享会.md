# 深度学习导论笔记

## 一、深度学习的定义与核心特点

### 1. 基本定义

深度学习（Deep Learning, DL）是机器学习（Machine Learning, ML）的一个分支，基于人工神经网络（Artificial Neural Networks, ANNs），通过**多层网络结构**对数据进行表征学习，自动提取数据的抽象特征

核心本质：用 “深度”（多隐藏层）模拟人类大脑神经元的层级传递过程，实现从 “数据” 到 “特征” 再到 “预测 / 决策” 的端到端学习

### 2. 与传统机器学习的差异

| 对比维度 | 传统机器学习                 | 深度学习                       |
| -------- | ---------------------------- | ------------------------------ |
| 特征提取 | 依赖人工设计（如 SIFT、HOG） | 自动从数据中学习特征           |
| 数据依赖 | 适用于小数据集（千 - 万级）  | 需大规模数据集（万 - 亿级）    |
| 计算需求 | 普通 CPU 可满足              | 依赖 GPU/TPU 等高性能计算设备  |
| 适用场景 | 简单分类、回归（如房价预测） | 复杂任务（图像识别、语音合成） |

## 二、深度学习发展历程（关键里程碑）

1. **1943 年：感知机理论奠基**

沃伦・麦卡洛克（Warren McCulloch）和沃尔特・皮茨（Walter Pitts）提出 “人工神经元模型”，模拟生物神经元的兴奋 / 抑制机制，是神经网络的雏形

2. **1986 年：反向传播算法突破**

杰弗里・辛顿（Geoffrey Hinton）等人提出反向传播（Backpropagation）算法，解决了多层神经网络的参数更新难题，推动深度学习初步发展

3. **2012 年：AlexNet 开启深度学习时代**

辛顿团队在 ImageNet 图像分类竞赛中，用 8 层 CNN（AlexNet）将错误率从 26% 降至 15%，证明深度学习在计算机视觉领域的优势，成为行业转折点

4. **2017 年：Transformer 架构诞生**

Google 团队提出 Transformer 模型，基于 “自注意力机制”（Self-Attention），解决 RNN 序列依赖问题，为 BERT、GPT 等大语言模型（LLM）奠定基础

5. **2022 年至今：大模型爆发期**

GPT-3（1750 亿参数）、GPT-4、文心一言等大语言模型涌现，深度学习从 “任务专用” 向 “通用智能” 迈进

## 三、深度学习核心基础

### 1. 必备数学知识

**线性代数** ：矩阵运算（神经网络层间数据传递本质是矩阵乘法）、向量空间（特征表示）、特征值分解（降维）

**概率论与数理统计** ：概率分布（如高斯分布用于初始化参数）、期望与方差（损失函数优化目标）、最大似然估计（模型参数求解）

**微积分** ：导数与偏导数（反向传播求梯度）、链式法则（多层网络梯度传递）、梯度下降（参数更新核心）

### 2. 人工神经网络基本结构

#### （1）神经元（Neuron）

结构：输入（\(x_1, x_2, ..., x_n\)）→ 权重（\(w_1, w_2, ..., w_n\)）→ 加权求和（\(z = \sum_{i=1}^n w_i x_i + b\)，\(b\)为偏置）→ 激活函数（\(a = f(z)\)）→ 输出

作用：单个神经元实现简单线性分类，多个神经元组合实现非线性映射

#### （2）网络层级

**输入层** ：接收原始数据（如图像的像素值、文本的词向量），神经元数量等于数据维度

**隐藏层** ：提取数据特征，层数（深度）和神经元数量需根据任务调整（如简单分类 1-2 层，复杂图像识别 10 + 层）

**输出层** ：输出模型结果（如分类任务的类别概率、回归任务的预测值），神经元数量等于任务目标维度

#### （3）常见网络类型

| 网络类型            | 核心特点                       | 适用场景                          |
| ------------------- | ------------------------------ | --------------------------------- |
| 卷积神经网络（CNN） | 局部感受野、权值共享，减少参数 | 图像识别（分类、检测）、视频分析  |
| 循环神经网络（RNN） | 时序依赖，处理序列数据         | 文本生成、语音识别、时间序列预测  |
| Transformer         | 自注意力机制，并行计算         | 大语言模型（GPT、BERT）、机器翻译 |

## 四、深度学习关键技术

### 1. 激活函数

作用：为神经网络引入非线性，使其能拟合复杂函数（无激活函数则多层网络等价于单层线性模型）

常用类型：

ReLU（Rectified Linear Unit）：\(f(z) = max(0, z)\)，解决梯度消失问题，计算高效

Sigmoid：\(f(z) = \frac{1}{1+e^{-z}}\)，输出映射到 [0,1]，适用于二分类输出层

Tanh：\(f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\)，输出映射到 [-1,1]，比 Sigmoid 更易训练

### 2. 优化器

作用：通过梯度下降调整网络参数（\(w, b\)），最小化损失函数

常用类型：

SGD（随机梯度下降）：每次用单个样本更新参数，训练速度快但波动大

Adam：结合动量（Momentum）和自适应学习率（RMSprop），收敛快、稳定性好，是当前主流优化器

Adagrad：对稀疏数据友好，自动调整不同参数的学习率

### 3. 损失函数

作用：衡量模型预测结果与真实标签的差距，是参数更新的 “指引信号”

常用类型：

均方误差（MSE）：\(L = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2\)，适用于回归任务

交叉熵损失（Cross-Entropy）：\(L = -\frac{1}{N}\sum_{i=1}^N y_i log\hat{y}_i\)，适用于分类任务（尤其是多分类）

### 4. 模型训练流程

1. **数据准备** ：采集数据→数据清洗（处理缺失值、异常值）→数据划分（训练集 70%、验证集 20%、测试集 10%）→数据增强（如图像翻转、裁剪，提升模型泛化能力）
2. **模型构建** ：定义网络结构（层数、神经元数量、激活函数）→初始化参数（如 Xavier 初始化、He 初始化）
3. **训练迭代** ：

前向传播：输入数据通过网络计算预测值\(\hat{y}\)

计算损失：用损失函数衡量\(\hat{y}\)与真实标签\(y\)的差距

反向传播：计算损失对各参数的梯度，用优化器更新参数

4. **模型评估与调优** ：用验证集评估模型性能（如准确率、F1-score）→调整超参数（学习率、 batch size、网络层数）→用测试集验证最终性能

## 五、深度学习典型应用领域

1. **计算机视觉（CV）**

核心任务：图像分类（如识别猫狗）、目标检测（如检测行人、车辆）、图像分割（如医学影像中分割肿瘤）、图像生成（如 GAN 生成逼真人脸）

代表应用：自动驾驶视觉感知、人脸识别考勤、AI 绘画（MidJourney）

2. **自然语言处理（NLP）**

核心任务：文本分类（垃圾邮件识别）、机器翻译（Google 翻译）、问答系统（ChatGPT）、情感分析（分析用户评论情绪）

代表应用：智能客服、语音助手（Siri）、文档自动摘要

3. **语音识别与合成**

语音识别：将语音信号转为文本（如微信语音转文字）

语音合成：将文本转为自然语音（如有声书、导航语音）

4. **其他领域**

推荐系统（如抖音推荐、电商商品推荐）、医疗诊断（AI 辅助识别 CT 影像）、金融风控（异常交易检测）

## 六、学习资源推荐

1. **经典书籍**

《深度学习》（Goodfellow 等著，被称为 “深度学习圣经”）

《动手学深度学习》（李沐等著，理论 + 代码实践结合）

《深度学习入门：基于 Python 的理论与实现》（适合零基础入门）

2. **在线课程**

Coursera：Andrew Ng《Deep Learning Specialization》（吴恩达经典课程）

B 站：李沐《动手学深度学习》（免费视频 + 代码讲解）

斯坦福大学 CS231n（计算机视觉）、CS224n（自然语言处理）

3. **实践工具**

框架：TensorFlow（Google，生态完善）、PyTorch（Facebook，灵活易上手，适合科研）

数据集：ImageNet（图像）、MNIST（手写数字）、IMDB（文本情感）、Kaggle（竞赛数据集平台）

可视化：TensorBoard（模型训练过程可视化）、Matplotlib（结果绘图）
